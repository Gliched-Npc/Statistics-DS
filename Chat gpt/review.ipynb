{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"IRIS.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.species.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"species\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(n_components=2)\n",
    "reduced_data=pca.fit_transform(df)\n",
    "print(\"Reduced Data Shape:\", reduced_data.shape)\n",
    "print(reduced_data)\n",
    "pca_data=pd.DataFrame(data=reduced_data,columns=['pca1','pca2'])\n",
    "pca_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.DataFrame(reduced_data,columns=['PC1','Pc2'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the Iris dataset\n",
    "df = pd.read_csv('iris.csv')  # Update the path to your dataset\n",
    "\n",
    "print(df.head(10))\n",
    "# Separate features and target\n",
    "X = df.drop('species', axis=1)  # Only features\n",
    "y = df['species']               # Target variable\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create a DataFrame with PCA results and target\n",
    "pca_df = pd.DataFrame(data=X_pca, columns=['Principal Component 1', 'Principal Component 2'])\n",
    "pca_df = pd.concat([pca_df, y.reset_index(drop=True)], axis=1)\n",
    "\n",
    "\n",
    "print(pca_df.head(10))\n",
    "\n",
    "# Plot the PCA results\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=pca_df, x='Principal Component 1', y='Principal Component 2', hue='species', palette='Set1')\n",
    "plt.title('PCA of IRIS Dataset')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(loc='best')  \n",
    "plt.show()\n",
    "\n",
    "# Print explained variance ratio\n",
    "print(\"Explained variance by each component:\", pca.explained_variance_ratio_)\n",
    "print(\"Total explained variance:\", sum(pca.explained_variance_ratio_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"IRIS.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# x=df.drop('species',axis=1)\n",
    "# y=df['species']\n",
    "\n",
    "# # Step 2: Standardize the features\n",
    "# x=StandardScaler().fit_transform(x)\n",
    "\n",
    "# # apply pca\n",
    "# pca=PCA(n_components=2)\n",
    "# x_pca=pca.fit_transform(x)\n",
    "\n",
    "# # Step 4: Create a DataFrame for PCA results\n",
    "# pca_df=pd.DataFrame(x_pca,columns=['pc1','pc2'])\n",
    "# pca_df['species']=y\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# encoder=LabelEncoder()\n",
    "# labeled=encoder.fit_transform(y)\n",
    "\n",
    "\n",
    "\n",
    "# # Step 5: Plot the results\n",
    "# plt.figure(figsize=(8,5))\n",
    "# plt.scatter(pca_df['pc1'],pca_df['pc2'],c=labeled,cmap=\"viridis\")\n",
    "# plt.title('PCA on Iris Dataset')\n",
    "# plt.xlabel('Principal Component 1')\n",
    "# plt.ylabel('Principal Component 2')\n",
    "# plt.colorbar(label='Species')\n",
    "# plt.show()\n",
    "# x_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# x=df.drop(\"species\",axis=1)\n",
    "# y=df['species']\n",
    "\n",
    "# pca=PCA(n_components=2)\n",
    "# x_pca=pca.fit_transform(x)\n",
    "\n",
    "# pca_df=pd.DataFrame(x_pca,columns=['pc1','pc2'])\n",
    "# pca_df['species']=y\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# encoder=LabelEncoder()\n",
    "# labeled=encoder.fit_transform(y)\n",
    "\n",
    "# plt.figure(figsize=(8,5))\n",
    "# plt.scatter(pca_df['pc1'],pca_df['pc2'],c=labeled,cmap=\"viridis\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "x=df.drop(\"species\",axis=1)\n",
    "y=df['species']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "x=StandardScaler().fit_transform(x)\n",
    "\n",
    "pca=PCA(n_components=2)\n",
    "x_pca=pca.fit_transform(x)\n",
    "\n",
    "df1=pd.DataFrame(x_pca,columns=['PC1','PC2'])\n",
    "df1['species']=y\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "enco=LabelEncoder()\n",
    "labeled_Y=enco.fit_transform(y)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(df1['PC1'],df1['PC2'],c=labeled_Y,cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "x=df.drop(\"species\",axis=1)\n",
    "y=df[\"species\"]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "x=StandardScaler().fit_transform(x)\n",
    "\n",
    "u,sigma,vt=np.linalg.svd(x)\n",
    "\n",
    "\n",
    "# svd=TruncatedSVD(n_components=2)\n",
    "# x_svd=svd.fit_transform(x)\n",
    "\n",
    "# svd_df=pd.DataFrame(x_svd,columns=['SVD Component 1', 'SVD Component 2'])\n",
    "# svd_df['species']=y\n",
    "# from  sklearn.preprocessing import LabelEncoder\n",
    "# encoder=LabelEncoder()\n",
    "# labeled=encoder.fit_transform(y)\n",
    "\n",
    "# plt.scatter(svd_df['SVD Component 1'],svd_df['SVD Component 2'],c=labeled,cmap='viridis')\n",
    "# svd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u,sigma,vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "x,y\n",
    "u,sigma,vt=np.linalg.svd(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 2: Load the Iris dataset and drop the species column (keep only numeric columns)\n",
    "df = pd.read_csv('iris.csv')  # Replace with your path\n",
    "X = df.drop(columns='species')\n",
    "\n",
    "# Step 3: Standardize the data (mean=0, variance=1)\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "# Step 4: Apply SVD using numpy\n",
    "U, Sigma, Vt = np.linalg.svd(X_standardized)\n",
    "\n",
    "# Step 5: Take the first two components to reduce dimensions to 2D\n",
    "X_svd_2d = X_standardized @ Vt.T[:, :2]\n",
    "\n",
    "# Step 6: Plot the 2D representation\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X_svd_2d[:, 0], X_svd_2d[:, 1], c=pd.factorize(df['species'])[0], cmap='viridis', edgecolor='k', s=50)\n",
    "plt.title(\"Iris Dataset in 2D (Using SVD)\")\n",
    "plt.xlabel(\"SVD Component 1\")\n",
    "plt.ylabel(\"SVD Component 2\")\n",
    "plt.colorbar(label='Species')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "U,Sigma,Vt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import libraries\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# # Load the dataset\n",
    "# df = pd.read_csv('iris.csv')  # Update with your path\n",
    "\n",
    "# # Encode the species column\n",
    "# label_encoder = LabelEncoder()\n",
    "# df['species_encoded'] = label_encoder.fit_transform(df['species'])\n",
    "\n",
    "# # Prepare features and target variable\n",
    "# X = df.drop(['species', 'species_encoded', 'petal_length'], axis=1)\n",
    "# y = df['petal_length']\n",
    "\n",
    "# # Standardize the features\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # Split the data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Train the model\n",
    "# model = LinearRegression()\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# print(f'Mean Squared Error: {mse:.2f}')\n",
    "# print(f'R-squared: {r2:.2f}')\n",
    "\n",
    "# # Check predictions\n",
    "# predictions_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "# print(\"\\nComparison of Actual vs Predicted values:\")\n",
    "# print(predictions_df.reset_index(drop=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummies=pd.get_dummies(df.species).astype(int)\n",
    "# dummies.head()\n",
    "\n",
    "\n",
    "# df1=pd.concat([df,dummies],axis='columns')\n",
    "# df1.drop(\"species\",axis=1,inplace=True)\n",
    "# df1\n",
    "\n",
    "# x=df1.drop('petal_length',axis=1)\n",
    "# y=df1['petal_length']\n",
    "\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=10)\n",
    "\n",
    "\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# li=LinearRegression()\n",
    "# li.fit(x_train,y_train)\n",
    "# li.score(x_test,y_test)\n",
    "\n",
    "\n",
    "# def predict_petal_lenght(sepal_length,sepal_width,petal_width,species):\n",
    "#     sep_index=np.where(x.columns==species)[0][0]\n",
    "\n",
    "#     a=np.zeros(len(x.columns))\n",
    "#     a[0]=sepal_length\n",
    "#     a[1]=sepal_width\n",
    "#     a[2]=petal_width\n",
    "\n",
    "#     if sep_index>=0:\n",
    "#         a[sep_index]=1\n",
    "#     return li.predict([a])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_petal_lenght(5.1,3.5,0.2,'Iris-setosa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies=pd.get_dummies(df['species']).astype(int)\n",
    "dummies\n",
    "new=pd.concat([df,dummies],axis=1)\n",
    "new.drop(\"species\",axis=1,inplace=True)\n",
    "new\n",
    "# predict the sepal length\n",
    "x=new.drop('sepal_length',axis=1)\n",
    "y=new['sepal_length']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,y_train,x_test,y_test=train_test_split(x,y,test_size=0.2,random_state=10)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "li=LinearRegression()\n",
    "li.fit(x_train,y_train)\n",
    "li.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# other way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import the necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 2: Load the Iris dataset\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv\"\n",
    "column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "iris = pd.read_csv(url, header=None, names=column_names)\n",
    "\n",
    "# Step 3: Prepare the data\n",
    "X = iris[['sepal_length', 'sepal_width']]  # Features\n",
    "y = iris['petal_length']                    # Target variable\n",
    "\n",
    "# Step 4: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Create and train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 7: Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'R-squared: {r2}')\n",
    "\n",
    "# Step 8: Visualize the results\n",
    "plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "plt.xlabel('Actual Petal Length')\n",
    "plt.ylabel('Predicted Petal Length')\n",
    "plt.legend(iris)\n",
    "plt.title('Actual vs Predicted Petal Length')\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], color='red', linestyle='--')  # Line for perfect predictions\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example data for covariance\n",
    "data = np.array([[2.5, 3.1, 2.8],\n",
    "                 [3.6, 3.9, 4.1],\n",
    "                 [4.0, 3.7, 3.3]])\n",
    "\n",
    "# Covariance matrix\n",
    "cov_matrix = np.cov(data, rowvar=False)\n",
    "print(\"Covariance Matrix:\\n\", cov_matrix)\n",
    "\n",
    "# Example matrix for transpose and inverse\n",
    "matrix = np.array([[4, 7],\n",
    "                   [2, 6]])\n",
    "\n",
    "# Transpose of the matrix    \n",
    "transpose_matrix = matrix.T\n",
    "print(\"Transpose Matrix:\\n\", transpose_matrix)\n",
    "\n",
    "# Inverse of the matrix\n",
    "inverse_matrix = np.linalg.inv(matrix)\n",
    "print(\"Inverse Matrix:\\n\", inverse_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Addition:\n",
      " [[ 6  8]\n",
      " [10 12]]\n",
      "Matrix Subtraction:\n",
      " [[-4 -4]\n",
      " [-4 -4]]\n",
      "Element-wise Multiplication:\n",
      " [[ 5 12]\n",
      " [21 32]]\n",
      "Dot Product:\n",
      " [[19 22]\n",
      " [43 50]]\n",
      "Transpose of A:\n",
      " [[1 3]\n",
      " [2 4]]\n",
      "Determinant of A: -2.0000000000000004\n",
      "Inverse of A:\n",
      " [[-2.   1. ]\n",
      " [ 1.5 -0.5]]\n",
      "Rank of A: 2\n",
      "Trace of A: 5\n",
      "Eigenvalues of A: [-0.37228132  5.37228132]\n",
      "Eigenvectors of A:\n",
      " [[-0.82456484 -0.41597356]\n",
      " [ 0.56576746 -0.90937671]]\n",
      "Covariance Matrix of data:\n",
      " [[4. 4.]\n",
      " [4. 4.]]\n",
      "SVD of A:\n",
      " [[-0.40455358 -0.9145143 ]\n",
      " [-0.9145143   0.40455358]] [5.4649857  0.36596619] [[-0.57604844 -0.81741556]\n",
      " [ 0.81741556 -0.57604844]]\n",
      "Solution for Ax=B:\n",
      " [-10.    9.5]\n",
      "Matrix Power (A^2):\n",
      " [[ 7 10]\n",
      " [15 22]]\n",
      "Norm of A: 5.477225575051661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_20156\\754356776.py:28: DeprecationWarning: Arrays of 2-dimensional vectors are deprecated. Use arrays of 3-dimensional vectors instead. (deprecated in NumPy 2.0)\n",
      "  np.cross(A,B)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-4, -4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example matrices\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "# Matrix operations\n",
    "print(\"Matrix Addition:\\n\", A + B)\n",
    "print(\"Matrix Subtraction:\\n\", A - B)\n",
    "print(\"Element-wise Multiplication:\\n\", A * B)\n",
    "print(\"Dot Product:\\n\", np.dot(A, B))\n",
    "print(\"Transpose of A:\\n\", A.T)\n",
    "print(\"Determinant of A:\", np.linalg.det(A))\n",
    "print(\"Inverse of A:\\n\", np.linalg.inv(A))\n",
    "print(\"Rank of A:\", np.linalg.matrix_rank(A))\n",
    "print(\"Trace of A:\", np.trace(A))\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "print(\"Eigenvalues of A:\", eigenvalues)\n",
    "print(\"Eigenvectors of A:\\n\", eigenvectors)\n",
    "print(\"Covariance Matrix of data:\\n\", np.cov(data, rowvar=False))\n",
    "U, S, V = np.linalg.svd(A)\n",
    "print(\"SVD of A:\\n\", U, S, V)\n",
    "print(\"Solution for Ax=B:\\n\", np.linalg.solve(A, np.array([9, 8])))\n",
    "print(\"Matrix Power (A^2):\\n\", np.linalg.matrix_power(A, 2))\n",
    "print(\"Norm of A:\", np.linalg.norm(A))\n",
    "\n",
    "np.cross(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 6, 8, 1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums=[2,1,3,4,6,8]\n",
    "arr1=[nums[0]]\n",
    "arr2=[nums[1]]\n",
    "for i in range(2,len(nums)):\n",
    "    if arr1[-1]>arr2[-1]:\n",
    "        arr1.append(nums[i])\n",
    "    else:\n",
    "        arr2.append(nums[i])\n",
    "arr1.extend(arr2)\n",
    "arr1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
